{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oWKN_zWhdP55",
        "outputId": "4e66fa9f-0f34-4589-f5a1-d1b44933da65"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Starting Data Preparation and EDA ---\n",
            "Step 0: Loading and Merging Datasets\n",
            "  - Successfully merged datasets. Total rows: 15010\n",
            "Step 1: Handling Missing and Inconsistent Values\n",
            "  - Imputed missing values in 'Balance' with median: 103202.74\n",
            "  - Imputed missing values in 'Tenure' with median: 5.00\n",
            "  - Imputed missing values in 'EstimatedSalary' with median: 154508.43\n",
            "  - Removed rows with missing 'Surname' or 'Gender'. New dataset size: 14960\n",
            "  - Removed 10 duplicate rows.\n",
            "\n",
            "Step 2: Performing Exploratory Data Analysis (EDA) and generating visualizations...\n",
            "  - Generating histograms for numerical features...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\c.muraw\\AppData\\Local\\Temp\\ipykernel_41960\\1155712422.py:55: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  df[col].fillna(median_val, inplace=True)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  - Generating box plots for outlier detection...\n",
            "  - Generating count plots for categorical features...\n",
            "\n",
            "Step 3: Standardizing and Encoding Data...\n",
            "--- Data Preparation and EDA Complete ---\n",
            "Final dataset shape: (14950, 15)\n",
            "\n",
            "Deliverable: A clean dataset ready for transformations:\n",
            "   num__CreditScore  num__Age  num__Tenure  num__Balance  num__NumOfProducts  \\\n",
            "0         -0.039196  0.124312    -1.013669     -1.335200           -1.394443   \n",
            "1         -0.125802  0.043188    -1.350455     -0.125025           -1.394443   \n",
            "2         -0.960374  0.124312     1.007046      0.970282            0.367244   \n",
            "3          0.590669 -0.119059    -1.350455     -1.335200           -0.513599   \n",
            "4          1.779540  0.205435    -1.013669      0.477160           -1.394443   \n",
            "\n",
            "   num__HasCrCard  num__IsActiveMember  num__EstimatedSalary  num__Exited  \\\n",
            "0        0.667295             0.977510             -0.581393     1.530205   \n",
            "1       -1.498588             0.977510             -0.459792    -0.653507   \n",
            "2        0.667295            -1.023007             -0.444703     1.530205   \n",
            "3       -1.498588            -1.023007             -0.663110    -0.653507   \n",
            "4        0.667295             0.977510             -0.823263    -0.653507   \n",
            "\n",
            "   cat__Geography_France  cat__Geography_Germany  cat__Geography_Spain  \\\n",
            "0                    1.0                     0.0                   0.0   \n",
            "1                    0.0                     0.0                   1.0   \n",
            "2                    1.0                     0.0                   0.0   \n",
            "3                    1.0                     0.0                   0.0   \n",
            "4                    0.0                     0.0                   1.0   \n",
            "\n",
            "   cat__Gender_Female  cat__Gender_Male  cat__Gender_nan  \n",
            "0                 1.0               0.0              0.0  \n",
            "1                 1.0               0.0              0.0  \n",
            "2                 1.0               0.0              0.0  \n",
            "3                 1.0               0.0              0.0  \n",
            "4                 1.0               0.0              0.0  \n"
          ]
        }
      ],
      "source": [
        "#%pip install pandas numpy scikit-learn matplotlib seaborn\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "def prepare_and_analyze_data():\n",
        "    \"\"\"\n",
        "    Combines two raw datasets, performs data cleaning, preprocessing, and\n",
        "    exploratory data analysis (EDA).\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A clean, preprocessed DataFrame.\n",
        "    \"\"\"\n",
        "\n",
        "    # Define the base directory for the data files.\n",
        "    data_dir = \"../3.RawDataStorage/hagging/\"\n",
        "    data_dirk= \"../3.RawDataStorage/kaggle/\"\n",
        "\n",
        "    # Define the full file paths using the base directory.\n",
        "    file_path_h = os.path.join(data_dir, \"h-bank_churn.csv\")\n",
        "    file_path_k = os.path.join(data_dirk, \"k-bank_churn.csv\")\n",
        "\n",
        "    # Check if the input files exist\n",
        "    if not os.path.exists(file_path_h) or not os.path.exists(file_path_k):\n",
        "        print(f\"Error: One or both of the files ('{file_path_h}' and '{file_path_k}') were not found.\")\n",
        "        return None\n",
        "\n",
        "    print(\"--- Starting Data Preparation and EDA ---\")\n",
        "\n",
        "    # Load and merge the raw data\n",
        "    print(\"Step 0: Loading and Merging Datasets\")\n",
        "    df_h = pd.read_csv(file_path_h)\n",
        "    df_k = pd.read_csv(file_path_k)\n",
        "    df = pd.concat([df_h, df_k], ignore_index=True)\n",
        "    print(f\"  - Successfully merged datasets. Total rows: {len(df)}\")\n",
        "\n",
        "    # 5.1 Clean and preprocess the raw data\n",
        "    print(\"Step 1: Handling Missing and Inconsistent Values\")\n",
        "\n",
        "    # Replace empty strings with NaN\n",
        "    df.replace('', np.nan, inplace=True)\n",
        "\n",
        "    # Impute numerical missing values with the median\n",
        "    numerical_cols = ['Balance', 'Tenure', 'EstimatedSalary']\n",
        "    for col in numerical_cols:\n",
        "        if df[col].isnull().sum() > 0:\n",
        "            median_val = df[col].median()\n",
        "            df[col].fillna(median_val, inplace=True)\n",
        "            print(f\"  - Imputed missing values in '{col}' with median: {median_val:.2f}\")\n",
        "\n",
        "    # Remove rows with missing values in critical categorical columns\n",
        "    df.dropna(subset=['Surname', 'Gender'], inplace=True)\n",
        "    print(f\"  - Removed rows with missing 'Surname' or 'Gender'. New dataset size: {len(df)}\")\n",
        "\n",
        "    # Handle inconsistent data (e.e.g., 'unknown' or invalid values)\n",
        "    df['Geography'] = df['Geography'].replace('unknown', 'France')\n",
        "    df['Gender'] = df['Gender'].replace('other', np.nan)\n",
        "    df['HasCrCard'] = df['HasCrCard'].replace(99, 1)\n",
        "\n",
        "    # Remove duplicates\n",
        "    initial_rows = len(df)\n",
        "    df.drop_duplicates(inplace=True)\n",
        "    duplicates_removed = initial_rows - len(df)\n",
        "    print(f\"  - Removed {duplicates_removed} duplicate rows.\")\n",
        "\n",
        "    # 5.2 Perform EDA to identify trends, distributions, and outliers.\n",
        "    print(\"\\nStep 2: Performing Exploratory Data Analysis (EDA) and generating visualizations...\")\n",
        "    os.makedirs('eda_visualizations', exist_ok=True)\n",
        "\n",
        "    # Visualizations for numerical data\n",
        "    print(\"  - Generating histograms for numerical features...\")\n",
        "    df.hist(figsize=(15, 10))\n",
        "    plt.suptitle(\"Histograms of Numerical Features\", y=0.95, fontsize=16)\n",
        "    plt.savefig(os.path.join('eda_visualizations', 'histograms.png'))\n",
        "    plt.close()\n",
        "\n",
        "    print(\"  - Generating box plots for outlier detection...\")\n",
        "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "    for i, col in enumerate(['CreditScore', 'Age', 'Tenure', 'Balance', 'NumOfProducts', 'EstimatedSalary']):\n",
        "        sns.boxplot(y=df[col], ax=axes[i//3, i%3])\n",
        "        axes[i//3, i%3].set_title(f'Box Plot of {col}')\n",
        "    plt.suptitle(\"Box Plots for Outlier Detection\", y=0.95, fontsize=16)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join('eda_visualizations', 'boxplots.png'))\n",
        "    plt.close()\n",
        "\n",
        "    # Visualizations for categorical data\n",
        "    print(\"  - Generating count plots for categorical features...\")\n",
        "    categorical_cols = ['Geography', 'Gender', 'HasCrCard', 'IsActiveMember', 'Exited']\n",
        "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "    for i, col in enumerate(categorical_cols):\n",
        "        sns.countplot(x=df[col], ax=axes[i//3, i%3])\n",
        "        axes[i//3, i%3].set_title(f'Count Plot of {col}')\n",
        "    plt.suptitle(\"Count Plots of Categorical Features\", y=0.95, fontsize=16)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join('eda_visualizations', 'count_plots.png'))\n",
        "    plt.close()\n",
        "\n",
        "    # 5.3 Standardize and encode data\n",
        "    print(\"\\nStep 3: Standardizing and Encoding Data...\")\n",
        "\n",
        "    # Identify feature types\n",
        "    numeric_features = df.select_dtypes(include=np.number).columns.tolist()\n",
        "    categorical_features = df.select_dtypes(include='object').columns.tolist()\n",
        "\n",
        "    # Drop non-essential columns from feature lists before dropping them from the DataFrame\n",
        "    cols_to_drop = ['RowNumber', 'CustomerId', 'Surname']\n",
        "    numeric_features = [col for col in numeric_features if col not in cols_to_drop]\n",
        "    categorical_features = [col for col in categorical_features if col not in cols_to_drop]\n",
        "\n",
        "    # Drop non-essential columns for modeling\n",
        "    df.drop(cols_to_drop, axis=1, inplace=True)\n",
        "\n",
        "    # Create preprocessing pipelines for numerical and categorical data\n",
        "    numeric_transformer = Pipeline(steps=[\n",
        "        ('imputer', SimpleImputer(strategy='median')), # Redundant but good practice\n",
        "        ('scaler', StandardScaler())\n",
        "    ])\n",
        "\n",
        "    categorical_transformer = Pipeline(steps=[\n",
        "        ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
        "    ])\n",
        "\n",
        "    # Create a preprocessor using ColumnTransformer\n",
        "    preprocessor = ColumnTransformer(\n",
        "        transformers=[\n",
        "            ('num', numeric_transformer, numeric_features),\n",
        "            ('cat', categorical_transformer, categorical_features)\n",
        "        ],\n",
        "        remainder='passthrough'\n",
        "    )\n",
        "\n",
        "    # Fit and transform the data\n",
        "    df_clean = preprocessor.fit_transform(df)\n",
        "\n",
        "    # Get feature names after one-hot encoding\n",
        "    new_features = preprocessor.get_feature_names_out()\n",
        "\n",
        "    df_clean = pd.DataFrame(df_clean, columns=new_features)\n",
        "\n",
        "    print(\"--- Data Preparation and EDA Complete ---\")\n",
        "    print(f\"Final dataset shape: {df_clean.shape}\")\n",
        "\n",
        "    return df_clean\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    clean_dataset = prepare_and_analyze_data()\n",
        "\n",
        "    if clean_dataset is not None:\n",
        "        print(\"\\nDeliverable: A clean dataset ready for transformations:\")\n",
        "        print(clean_dataset.head())\n",
        "        # You can save this cleaned dataset if needed\n",
        "        clean_dataset.to_csv('cleaned_data.csv', index=False)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.12.7 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    },
    "vscode": {
      "interpreter": {
        "hash": "faa1fa619533ebb939c61fcbcc7a64e69ab58da1fd275013ca19dbaacbd28041"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
