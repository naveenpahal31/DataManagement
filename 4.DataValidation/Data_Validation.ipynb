{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4de33515-17f7-415c-971c-c2efe218d06b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting data validation with time: 2025-08-23 20:46:29\n",
      "--------------------------------------------------\n",
      "Step 1: Ingesting and Merging Data\n",
      "Successfully merged 10000 rows from 'h-bank_churn.csv' and 5010 rows from 'k-bank_churn.csv'.\n",
      "Total rows in combined dataset: 15010\n",
      "--------------------------------------------------\n",
      "Step 2: Checking for Missing Data (NaN values)\n",
      "Missing values found:\n",
      "Surname             50\n",
      "Tenure             101\n",
      "Balance            100\n",
      "EstimatedSalary    100\n",
      "dtype: int64\n",
      "--------------------------------------------------\n",
      "Step 3: Validating Data Types and Initial Structure\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 15010 entries, 0 to 15009\n",
      "Data columns (total 14 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   RowNumber        15010 non-null  int64  \n",
      " 1   CustomerId       15010 non-null  int64  \n",
      " 2   Surname          14960 non-null  object \n",
      " 3   CreditScore      15010 non-null  int64  \n",
      " 4   Geography        15010 non-null  object \n",
      " 5   Gender           15010 non-null  object \n",
      " 6   Age              15010 non-null  int64  \n",
      " 7   Tenure           14909 non-null  float64\n",
      " 8   Balance          14910 non-null  float64\n",
      " 9   NumOfProducts    15010 non-null  int64  \n",
      " 10  HasCrCard        15010 non-null  int64  \n",
      " 11  IsActiveMember   15010 non-null  int64  \n",
      " 12  EstimatedSalary  14910 non-null  float64\n",
      " 13  Exited           15010 non-null  int64  \n",
      "dtypes: float64(3), int64(8), object(3)\n",
      "memory usage: 1.6+ MB\n",
      "None\n",
      "--------------------------------------------------\n",
      "Step 4: Checking for Inconsistent and Outlier Data\n",
      "\n",
      "Descriptive statistics for 'CreditScore':\n",
      "count    15010.000000\n",
      "mean       623.886476\n",
      "std        127.244209\n",
      "min        300.000000\n",
      "25%        543.000000\n",
      "50%        637.000000\n",
      "75%        716.000000\n",
      "max        850.000000\n",
      "Name: CreditScore, dtype: float64\n",
      "\n",
      "Descriptive statistics for 'Age':\n",
      "count    15010.000000\n",
      "mean        40.481013\n",
      "std         12.347650\n",
      "min         18.000000\n",
      "25%         32.000000\n",
      "50%         38.000000\n",
      "75%         48.000000\n",
      "max         92.000000\n",
      "Name: Age, dtype: float64\n",
      "Found 85 outliers in 'Age' based on IQR method.\n",
      "\n",
      "Descriptive statistics for 'Tenure':\n",
      "count    14909.000000\n",
      "mean         5.011872\n",
      "std          2.978986\n",
      "min          0.000000\n",
      "25%          2.000000\n",
      "50%          5.000000\n",
      "75%          8.000000\n",
      "max         10.000000\n",
      "Name: Tenure, dtype: float64\n",
      "\n",
      "Descriptive statistics for 'Balance':\n",
      "count     14910.000000\n",
      "mean      92577.732145\n",
      "std       69528.224802\n",
      "min           0.000000\n",
      "25%        5357.841750\n",
      "50%      103202.735000\n",
      "75%      139993.594129\n",
      "max      250898.090000\n",
      "Name: Balance, dtype: float64\n",
      "\n",
      "Descriptive statistics for 'NumOfProducts':\n",
      "count    15010.000000\n",
      "mean         2.582678\n",
      "std          1.134851\n",
      "min          1.000000\n",
      "25%          2.000000\n",
      "50%          3.000000\n",
      "75%          4.000000\n",
      "max          6.000000\n",
      "Name: NumOfProducts, dtype: float64\n",
      "\n",
      "Descriptive statistics for 'EstimatedSalary':\n",
      "count     14910.000000\n",
      "mean     154649.074038\n",
      "std       92313.420342\n",
      "min           8.059242\n",
      "25%       78627.028372\n",
      "50%      154508.430000\n",
      "75%      213450.752500\n",
      "max      349953.330000\n",
      "Name: EstimatedSalary, dtype: float64\n",
      "\n",
      "Value counts for categorical columns:\n",
      "\n",
      "'Geography':\n",
      "Geography\n",
      "France     6303\n",
      "Germany    3785\n",
      "Spain      3748\n",
      "unknown    1174\n",
      "Name: count, dtype: int64\n",
      "\n",
      "'Gender':\n",
      "Gender\n",
      "Male      7100\n",
      "Female    6244\n",
      "other     1666\n",
      "Name: count, dtype: int64\n",
      "\n",
      "'HasCrCard':\n",
      "HasCrCard\n",
      "1     8728\n",
      "0     4625\n",
      "99    1657\n",
      "Name: count, dtype: int64\n",
      "\n",
      "'IsActiveMember':\n",
      "IsActiveMember\n",
      "1    7677\n",
      "0    7333\n",
      "Name: count, dtype: int64\n",
      "\n",
      "'Exited':\n",
      "Exited\n",
      "0    10504\n",
      "1     4506\n",
      "Name: count, dtype: int64\n",
      "--------------------------------------------------\n",
      "Step 5: Identifying Duplicate Rows\n",
      "Found 10 duplicate rows.\n",
      "--------------------------------------------------\n",
      "Validation complete. Generating data quality report...\n",
      "\n",
      "Data quality report saved to c:\\Data_Management\\4.Data Validation\\data_quality_report.md\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#%pip install pandas numpy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Define the base directory for the data files.\n",
    "data_dir = \"../3.RawDataStorage/hagging/\"\n",
    "data_dirk= \"../3.RawDataStorage/kaggle/\"\n",
    "\n",
    "# Define the full file paths using the base directory.\n",
    "file_path_h = os.path.join(data_dir, \"h-bank_churn.csv\")\n",
    "file_path_k = os.path.join(data_dirk,\"k-bank_churn.csv\")\n",
    "\n",
    "def run_data_validation():\n",
    "    \"\"\"\n",
    "    Combines two datasets and performs a series of data quality checks,\n",
    "    then generates a summary report.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize a list to store report findings\n",
    "    report_data = []\n",
    "\n",
    "    def add_report_finding(category, description):\n",
    "        \"\"\"Helper function to add findings to the report.\"\"\"\n",
    "        report_data.append({\"Category\": category, \"Description\": description})\n",
    "\n",
    "    print(f\"Starting data validation with time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    print(\"-\" * 50)\n",
    "    print(\"Step 1: Ingesting and Merging Data\")\n",
    "\n",
    "    try:\n",
    "        # Load the two CSV files into pandas DataFrames\n",
    "        df_h = pd.read_csv(file_path_h)\n",
    "        df_k = pd.read_csv(file_path_k)\n",
    "        \n",
    "        # Merge the two DataFrames\n",
    "        df = pd.concat([df_h, df_k], ignore_index=True)\n",
    "\n",
    "        print(f\"Successfully merged {len(df_h)} rows from 'h-bank_churn.csv' and \"\n",
    "              f\"{len(df_k)} rows from 'k-bank_churn.csv'.\")\n",
    "        print(f\"Total rows in combined dataset: {len(df)}\")\n",
    "        print(\"-\" * 50)\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Error: One of the files was not found. Please ensure both '{file_path_h}' and \"\n",
    "              f\"'{file_path_k}' are in the correct directory.\")\n",
    "        add_report_finding(\"File Ingestion Error\", f\"Could not find required data file: {e}\")\n",
    "        return\n",
    "\n",
    "    print(\"Step 2: Checking for Missing Data (NaN values)\")\n",
    "    # Check for missing values in each column\n",
    "    missing_data = df.isnull().sum()\n",
    "    if missing_data.sum() > 0:\n",
    "        print(\"Missing values found:\")\n",
    "        print(missing_data[missing_data > 0])\n",
    "        add_report_finding(\"Missing Data\", \"Missing values found in the following columns: \"\n",
    "                           f\"{missing_data[missing_data > 0].to_dict()}\")\n",
    "    else:\n",
    "        print(\"No missing values found. Data is complete.\")\n",
    "        add_report_finding(\"Missing Data\", \"No missing values found.\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    print(\"Step 3: Validating Data Types and Initial Structure\")\n",
    "    # Display data types and non-null counts\n",
    "    print(df.info())\n",
    "    add_report_finding(\"Data Types\", \"Initial data types and structure verified. See log output for details.\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    print(\"Step 4: Checking for Inconsistent and Outlier Data\")\n",
    "    # Validate numerical columns for logical ranges and outliers\n",
    "    numeric_cols = ['CreditScore', 'Age', 'Tenure', 'Balance', 'NumOfProducts', 'EstimatedSalary']\n",
    "    \n",
    "    for col in numeric_cols:\n",
    "        desc = df[col].describe()\n",
    "        print(f\"\\nDescriptive statistics for '{col}':\\n{desc}\")\n",
    "        # Identify outliers using IQR\n",
    "        Q1 = desc['25%']\n",
    "        Q3 = desc['75%']\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        outliers = df[(df[col] < lower_bound) | (df[col] > upper_bound)]\n",
    "        if not outliers.empty:\n",
    "            print(f\"Found {len(outliers)} outliers in '{col}' based on IQR method.\")\n",
    "            add_report_finding(\"Outliers\", f\"Found {len(outliers)} outliers in column '{col}'. \"\n",
    "                               f\"Example: {outliers[col].head().tolist()}\")\n",
    "        \n",
    "    # Check for inconsistent values in categorical columns\n",
    "    categorical_cols = ['Geography', 'Gender', 'HasCrCard', 'IsActiveMember', 'Exited']\n",
    "    print(\"\\nValue counts for categorical columns:\")\n",
    "    for col in categorical_cols:\n",
    "        counts = df[col].value_counts()\n",
    "        print(f\"\\n'{col}':\\n{counts}\")\n",
    "        if col in ['HasCrCard', 'IsActiveMember', 'Exited'] and not set(counts.index).issubset({0, 1}):\n",
    "            add_report_finding(\"Inconsistent Data\", f\"Column '{col}' contains values other than 0 or 1.\")\n",
    "        \n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    print(\"Step 5: Identifying Duplicate Rows\")\n",
    "    # Check for duplicate rows\n",
    "    duplicate_rows = df.duplicated().sum()\n",
    "    if duplicate_rows > 0:\n",
    "        print(f\"Found {duplicate_rows} duplicate rows.\")\n",
    "        add_report_finding(\"Duplicates\", f\"Found {duplicate_rows} duplicate rows in the dataset.\")\n",
    "    else:\n",
    "        print(\"No duplicate rows found.\")\n",
    "        add_report_finding(\"Duplicates\", \"No duplicate rows found.\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    print(\"Validation complete. Generating data quality report...\")\n",
    "    \n",
    "    # Generate the comprehensive data quality report\n",
    "    report_df = pd.DataFrame(report_data)\n",
    "    \n",
    "    # Save the report to a Markdown file\n",
    "    report_file = os.path.join(os.getcwd(), \"data_quality_report.md\")\n",
    "    with open(report_file, \"w\") as f:\n",
    "        f.write(\"# Data Quality Report\\n\\n\")\n",
    "        f.write(f\"**Report Generated:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\\n\")\n",
    "        f.write(\"This report summarizes the data quality findings from the `bank_churn` dataset.\\n\\n\")\n",
    "        f.write(\"## Dataset Overview\\n\")\n",
    "        f.write(f\"- Total Rows: {len(df)}\\n\")\n",
    "        f.write(f\"- Total Columns: {len(df.columns)}\\n\\n\")\n",
    "        \n",
    "        f.write(\"## Validation Findings Summary\\n\")\n",
    "        f.write(report_df.to_markdown(index=False))\n",
    "\n",
    "    print(f\"\\nData quality report saved to {report_file}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "\n",
    "# Execute the validation process\n",
    "if __name__ == \"__main__\":\n",
    "    run_data_validation()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "faa1fa619533ebb939c61fcbcc7a64e69ab58da1fd275013ca19dbaacbd28041"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
